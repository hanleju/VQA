{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cabbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Penguin\\anaconda3\\envs\\vqa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import (\n",
    "    ViltProcessor, ViltForQuestionAnswering,\n",
    "    BlipProcessor, BlipForQuestionAnswering,\n",
    "    BertTokenizer\n",
    ")\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from data import VQADataset, collate_fn_with_tokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63616b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "dataset_root = 'D:/VQA/cocoqa'\n",
    "model_type = 'vilt'  # 'vilt' or 'blip'\n",
    "trained_model_path = './consistency/consistency_models/vilt/pytorch_model.pth'  # Trained model path\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "max_samples = 1000\n",
    "perplexity = 30\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd725a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_and_losses(model, processor, model_name, dataloader, \n",
    "                                  dataset_root, split, device, max_samples=2000):\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings_list = []\n",
    "    losses_list = []\n",
    "    labels_list = []\n",
    "    predictions_list = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    print(f\"Extracting embeddings from {model_name}...\")\n",
    "    \n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting\", leave=False):\n",
    "            if num_samples >= max_samples:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                image_paths = []\n",
    "                for img_id in batch['image_id']:\n",
    "                    base_path = os.path.join(dataset_root, split, 'images', img_id)\n",
    "                    found = False\n",
    "                    for ext in ['.jpg', '.png', '.jpeg']:\n",
    "                        if os.path.exists(base_path + ext):\n",
    "                            image_paths.append(base_path + ext)\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        if os.path.exists(base_path):\n",
    "                            image_paths.append(base_path)\n",
    "                        else:\n",
    "                            continue\n",
    "                \n",
    "                if len(image_paths) == 0:\n",
    "                    continue\n",
    "                \n",
    "                images = [Image.open(path).convert('RGB') for path in image_paths]\n",
    "                questions = batch['question'][:len(images)]\n",
    "                answers = batch['answer'][:len(images)].to(device)\n",
    "                answer_texts = batch['answer_text'][:len(images)] if 'answer_text' in batch else [str(a.item()) for a in answers]\n",
    "                \n",
    "                if \"blip\" in model_name.lower():\n",
    "                    inputs = processor(images=images, text=questions, return_tensors=\"pt\", \n",
    "                                     padding=True, truncation=True).to(device)\n",
    "                    labels = processor(text=answer_texts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "                    outputs = model(**inputs, labels=labels)\n",
    "                    \n",
    "                    batch_loss = outputs.loss.item()\n",
    "                    losses_batch = np.array([batch_loss] * len(images))\n",
    "                    embeddings_batch = outputs.logits[:, 0, :].cpu().numpy()\n",
    "                    predictions_batch = torch.argmax(outputs.logits[:, 0, :], dim=-1).cpu().numpy()\n",
    "                    \n",
    "                elif \"vilt\" in model_name.lower():\n",
    "                    inputs = processor(images=images, text=questions, return_tensors=\"pt\", \n",
    "                                     padding=True, truncation=True, max_length=40).to(device)\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    \n",
    "                    losses_batch = criterion(logits, answers).cpu().numpy()\n",
    "                    embeddings_batch = logits.cpu().numpy()\n",
    "                    predictions_batch = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "                \n",
    "                embeddings_list.append(embeddings_batch)\n",
    "                losses_list.append(losses_batch)\n",
    "                labels_list.append(answers.cpu().numpy())\n",
    "                predictions_list.append(predictions_batch)\n",
    "                \n",
    "                num_samples += len(images)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n⚠ Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    losses = np.concatenate(losses_list)\n",
    "    labels = np.concatenate(labels_list)\n",
    "    predictions = np.concatenate(predictions_list)\n",
    "    \n",
    "    print(f\"✓ Extracted {len(embeddings)} samples\")\n",
    "    print(f\"  Embedding shape: {embeddings.shape}\")\n",
    "    print(f\"  Loss range: [{losses.min():.2f}, {losses.max():.2f}]\")\n",
    "    \n",
    "    return embeddings, losses, labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73384b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_difficulty_groups(losses):\n",
    "    p33 = np.percentile(losses, 33)\n",
    "    p66 = np.percentile(losses, 66)\n",
    "    \n",
    "    groups = np.zeros(len(losses), dtype=int)\n",
    "    groups[losses >= p33] = 1\n",
    "    groups[losses >= p66] = 2\n",
    "    \n",
    "    print(f\"\\nDifficulty Groups:\")\n",
    "    print(f\"  Easy (loss < {p33:.2f}): {np.sum(groups == 0)} samples\")\n",
    "    print(f\"  Medium ({p33:.2f} <= loss < {p66:.2f}): {np.sum(groups == 1)} samples\")\n",
    "    print(f\"  Hard (loss >= {p66:.2f}): {np.sum(groups == 2)} samples\")\n",
    "    \n",
    "    return groups, p33, p66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8529be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(embeddings, losses, groups, labels, predictions, title, perplexity=30):\n",
    "    print(f\"\\nPerforming t-SNE projection...\")\n",
    "    \n",
    "    if embeddings.shape[1] > 50:\n",
    "        print(f\"  PCA: {embeddings.shape[1]} -> 50\")\n",
    "        pca = PCA(n_components=50)\n",
    "        embeddings_pca = pca.fit_transform(embeddings)\n",
    "        print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    else:\n",
    "        embeddings_pca = embeddings\n",
    "    \n",
    "    print(f\"  t-SNE: 50 -> 2 (perplexity={perplexity})\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, \n",
    "                n_iter=1000, verbose=0)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_pca)\n",
    "    \n",
    "    print(f\"✓ t-SNE completed\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    \n",
    "    difficulty_names = ['Easy', 'Medium', 'Hard']\n",
    "    difficulty_colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "    \n",
    "    # 1. Difficulty Groups\n",
    "    ax = axes[0, 0]\n",
    "    for group_idx in range(3):\n",
    "        mask = (groups == group_idx)\n",
    "        ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "                  c=difficulty_colors[group_idx], label=difficulty_names[group_idx],\n",
    "                  alpha=0.6, s=30, edgecolors='none')\n",
    "    \n",
    "    ax.set_title(f'Output Distribution by Difficulty\\n{title}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    ax.legend(fontsize=11, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Loss Heatmap\n",
    "    ax = axes[0, 1]\n",
    "    scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "                        c=losses, cmap='RdYlGn_r', alpha=0.6, s=30, \n",
    "                        edgecolors='none', vmin=losses.min(), vmax=losses.max())\n",
    "    ax.set_title('Loss Distribution (Continuous)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Loss', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Correctness\n",
    "    ax = axes[1, 0]\n",
    "    correct = (labels == predictions)\n",
    "    ax.scatter(embeddings_2d[~correct, 0], embeddings_2d[~correct, 1],\n",
    "              c='#e74c3c', label='Incorrect', alpha=0.6, s=30, edgecolors='none')\n",
    "    ax.scatter(embeddings_2d[correct, 0], embeddings_2d[correct, 1],\n",
    "              c='#2ecc71', label='Correct', alpha=0.6, s=30, edgecolors='none')\n",
    "    ax.set_title('Prediction Correctness', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    ax.legend(fontsize=11, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Statistics\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    stats_text = f\"Statistics:\\n\\n\"\n",
    "    stats_text += f\"Total Samples: {len(embeddings)}\\n\\n\"\n",
    "    \n",
    "    for group_idx in range(3):\n",
    "        mask = (groups == group_idx)\n",
    "        group_losses = losses[mask]\n",
    "        group_correct = correct[mask]\n",
    "        \n",
    "        stats_text += f\"{difficulty_names[group_idx]} Group:\\n\"\n",
    "        stats_text += f\"  Count: {np.sum(mask)}\\n\"\n",
    "        stats_text += f\"  Mean Loss: {group_losses.mean():.3f}\\n\"\n",
    "        stats_text += f\"  Std Loss: {group_losses.std():.3f}\\n\"\n",
    "        stats_text += f\"  Accuracy: {group_correct.mean():.2%}\\n\\n\"\n",
    "    \n",
    "    stats_text += f\"Overall:\\n\"\n",
    "    stats_text += f\"  Mean Loss: {losses.mean():.3f}\\n\"\n",
    "    stats_text += f\"  Std Loss: {losses.std():.3f}\\n\"\n",
    "    stats_text += f\"  Accuracy: {correct.mean():.2%}\\n\"\n",
    "    \n",
    "    ax.text(0.1, 0.9, stats_text, transform=ax.transAxes,\n",
    "           fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fcbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_before_after(original_data, trained_data, original_tsne, trained_tsne):\n",
    "    orig_emb, orig_losses, orig_groups, orig_labels, orig_preds = original_data\n",
    "    train_emb, train_losses, train_groups, train_labels, train_preds = trained_data\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    difficulty_names = ['Easy', 'Medium', 'Hard']\n",
    "    difficulty_colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "    \n",
    "    # Row 1: Original Model\n",
    "    ax = axes[0, 0]\n",
    "    for group_idx in range(3):\n",
    "        mask = (orig_groups == group_idx)\n",
    "        ax.scatter(original_tsne[mask, 0], original_tsne[mask, 1],\n",
    "                  c=difficulty_colors[group_idx], label=difficulty_names[group_idx],\n",
    "                  alpha=0.6, s=20, edgecolors='none')\n",
    "    ax.set_title('BEFORE: Difficulty Groups', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('t-SNE Dim 1', fontsize=10)\n",
    "    ax.set_ylabel('t-SNE Dim 2', fontsize=10)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    scatter = ax.scatter(original_tsne[:, 0], original_tsne[:, 1],\n",
    "                        c=orig_losses, cmap='RdYlGn_r', alpha=0.6, s=20, edgecolors='none')\n",
    "    ax.set_title('BEFORE: Loss Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('t-SNE Dim 1', fontsize=10)\n",
    "    ax.set_ylabel('t-SNE Dim 2', fontsize=10)\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 2]\n",
    "    ax.axis('off')\n",
    "    orig_stats = f\"BEFORE Statistics:\\n\\n\"\n",
    "    for group_idx in range(3):\n",
    "        mask = (orig_groups == group_idx)\n",
    "        group_losses = orig_losses[mask]\n",
    "        orig_stats += f\"{difficulty_names[group_idx]}:\\n\"\n",
    "        orig_stats += f\"  Mean: {group_losses.mean():.3f}\\n\"\n",
    "        orig_stats += f\"  Std: {group_losses.std():.3f}\\n\\n\"\n",
    "    ax.text(0.1, 0.9, orig_stats, transform=ax.transAxes,\n",
    "           fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "    \n",
    "    # Row 2: Trained Model\n",
    "    ax = axes[1, 0]\n",
    "    for group_idx in range(3):\n",
    "        mask = (train_groups == group_idx)\n",
    "        ax.scatter(trained_tsne[mask, 0], trained_tsne[mask, 1],\n",
    "                  c=difficulty_colors[group_idx], label=difficulty_names[group_idx],\n",
    "                  alpha=0.6, s=20, edgecolors='none')\n",
    "    ax.set_title('AFTER: Difficulty Groups', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('t-SNE Dim 1', fontsize=10)\n",
    "    ax.set_ylabel('t-SNE Dim 2', fontsize=10)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    scatter = ax.scatter(trained_tsne[:, 0], trained_tsne[:, 1],\n",
    "                        c=train_losses, cmap='RdYlGn_r', alpha=0.6, s=20, edgecolors='none')\n",
    "    ax.set_title('AFTER: Loss Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('t-SNE Dim 1', fontsize=10)\n",
    "    ax.set_ylabel('t-SNE Dim 2', fontsize=10)\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 2]\n",
    "    ax.axis('off')\n",
    "    train_stats = f\"AFTER Statistics:\\n\\n\"\n",
    "    for group_idx in range(3):\n",
    "        mask = (train_groups == group_idx)\n",
    "        group_losses = train_losses[mask]\n",
    "        train_stats += f\"{difficulty_names[group_idx]}:\\n\"\n",
    "        train_stats += f\"  Mean: {group_losses.mean():.3f}\\n\"\n",
    "        train_stats += f\"  Std: {group_losses.std():.3f}\\n\\n\"\n",
    "    \n",
    "    train_stats += f\"\\nImprovement:\\n\"\n",
    "    for group_idx in range(3):\n",
    "        mask_orig = (orig_groups == group_idx)\n",
    "        mask_train = (train_groups == group_idx)\n",
    "        std_orig = orig_losses[mask_orig].std()\n",
    "        std_train = train_losses[mask_train].std()\n",
    "        improvement = (std_orig - std_train) / std_orig * 100\n",
    "        train_stats += f\"{difficulty_names[group_idx]} Std: {improvement:+.1f}%\\n\"\n",
    "    \n",
    "    ax.text(0.1, 0.9, train_stats, transform=ax.transAxes,\n",
    "           fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "           bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4588d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "collate_fn = partial(collate_fn_with_tokenizer, tokenizer=tokenizer)\n",
    "\n",
    "dataset = VQADataset(root_dir=dataset_root, split='train', transform=image_transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c087ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "if model_type == 'vilt':\n",
    "    pretrained_name = 'dandelin/vilt-b32-finetuned-vqa'\n",
    "    processor = ViltProcessor.from_pretrained(pretrained_name)\n",
    "    original_model = ViltForQuestionAnswering.from_pretrained(pretrained_name, use_safetensors=True).to(device)\n",
    "elif model_type == 'blip':\n",
    "    pretrained_name = 'Salesforce/blip-vqa-base'\n",
    "    processor = BlipProcessor.from_pretrained(pretrained_name)\n",
    "    original_model = BlipForQuestionAnswering.from_pretrained(pretrained_name).to(device)\n",
    "\n",
    "print(f\"Original model loaded: {pretrained_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract original model embeddings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing ORIGINAL model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "orig_emb, orig_losses, orig_labels, orig_preds = extract_embeddings_and_losses(\n",
    "    original_model, processor, pretrained_name, dataloader,\n",
    "    dataset_root, 'train', device, max_samples\n",
    ")\n",
    "\n",
    "orig_groups, orig_p33, orig_p66 = assign_difficulty_groups(orig_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b71f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Visualizing ORIGINAL model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "original_tsne = visualize_tsne(\n",
    "    orig_emb, orig_losses, orig_groups, orig_labels, orig_preds,\n",
    "    title=f\"Original {model_type.upper()} Model\",\n",
    "    perplexity=perplexity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86691cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "if os.path.exists(trained_model_path):\n",
    "    print(f\"\\nLoading trained model from: {trained_model_path}\")\n",
    "    \n",
    "    if model_type == 'vilt':\n",
    "        trained_model = ViltForQuestionAnswering.from_pretrained(pretrained_name, use_safetensors=True)\n",
    "        trained_model.load_state_dict(torch.load(trained_model_path, map_location=device))\n",
    "        trained_model = trained_model.to(device)\n",
    "    elif model_type == 'blip':\n",
    "        trained_model = BlipForQuestionAnswering.from_pretrained(pretrained_name)\n",
    "        trained_model.load_state_dict(torch.load(trained_model_path, map_location=device))\n",
    "        trained_model = trained_model.to(device)\n",
    "    \n",
    "    print(\"✓ Trained model loaded successfully\")\n",
    "else:\n",
    "    print(f\"\\n⚠ WARNING: Trained model not found at {trained_model_path}\")\n",
    "    print(\"Please train the model first using consistency_train.py\")\n",
    "    trained_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trained model embeddings\n",
    "if trained_model is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Processing TRAINED model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_emb, train_losses, train_labels, train_preds = extract_embeddings_and_losses(\n",
    "        trained_model, processor, f\"Trained {model_type}\", dataloader,\n",
    "        dataset_root, 'train', device, max_samples\n",
    "    )\n",
    "    \n",
    "    train_groups, train_p33, train_p66 = assign_difficulty_groups(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trained model\n",
    "if trained_model is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Visualizing TRAINED model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    trained_tsne = visualize_tsne(\n",
    "        train_emb, train_losses, train_groups, train_labels, train_preds,\n",
    "        title=f\"Consistency-Trained {model_type.upper()} Model\",\n",
    "        perplexity=perplexity\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f94ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after\n",
    "if trained_model is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Creating COMPARISON visualization...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    compare_before_after(\n",
    "        (orig_emb, orig_losses, orig_groups, orig_labels, orig_preds),\n",
    "        (train_emb, train_losses, train_groups, train_labels, train_preds),\n",
    "        original_tsne,\n",
    "        trained_tsne\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Visualization completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
